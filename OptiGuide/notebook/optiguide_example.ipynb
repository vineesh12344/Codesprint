{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a461c72d",
      "metadata": {
        "id": "a461c72d"
      },
      "source": [
        "# OptiGuide Example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a05fc7",
      "metadata": {
        "id": "59a05fc7"
      },
      "source": [
        "Here we give a simple example, as designed and illustrated in the [OptiGuide paper](https://arxiv.org/abs/2307.03875).\n",
        "While the original paper is designed specifically for supply chain optimization, the general framework can be easily adapted to other applications with coding capacity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e92d200",
      "metadata": {
        "id": "5e92d200"
      },
      "source": [
        "## OptiGuide for Supply Chain Optimization: System Design Overview\n",
        "\n",
        "The original system design for OptiGuide, tailored for supply chain optimization, is presented below.\n",
        "\n",
        "The collaboration among three agents -- Coder, Safeguard, and Interpreter -- lies at the core of this system. They leverage a set of external tools and a large language model (LLM) to address users' questions related to supply chain applications. For a comprehensive understanding of the design and data flow, detailed information can be found in the original [paper](https://arxiv.org/abs/2307.03875).\n",
        "\n",
        "\n",
        "![optiguide system](https://www.beibinli.com/docs/optiguide/optiguide_system.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7f90c8",
      "metadata": {
        "id": "8b7f90c8"
      },
      "source": [
        "## New Implementation\n",
        "\n",
        "\n",
        "\n",
        "![](new_design.png)\n",
        "\n",
        "Advantages of this multi-agent design with autogen:\n",
        "- Collaborative Problem Solving: The collaboration among the user proxy agent and the assistant agents fosters a cooperative problem-solving environment. The agents can share information and knowledge, allowing them to complement each other's abilities and collectively arrive at better solutions. On the other hand, the Safeguard acts as a virtual adversarial checker, which can perform another safety check pass on the generated code.\n",
        "\n",
        "- Modularity: The division of tasks into separate agents promotes modularity in the system. Each agent can be developed, tested, and maintained independently, simplifying the overall development process and facilitating code management.\n",
        "\n",
        "- Memory Management: The OptiGuide agent's role in maintaining memory related to user interactions is crucial. The memory retention allows the agents to have context about a user's prior questions, making the decision-making process more informed and context-aware.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ReAgLnDma3oI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAgLnDma3oI",
        "outputId": "c95a23e8-ddb9-4fd2-f218-c40a51cbcbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install Required Packages\n",
        "%pip install optiguide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3b79c4",
      "metadata": {
        "id": "9a3b79c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test Gurobi installation\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from eventlet.timeout import Timeout\n",
        "\n",
        "# import auxillary packages\n",
        "import re\n",
        "import requests  # for loading the example source code\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# import flaml and autogen\n",
        "from flaml import autogen\n",
        "from flaml.autogen.agentchat import Agent, UserProxyAgent\n",
        "from optiguide.optiguide import OptiGuideAgent\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "# Path to\n",
        "load_dotenv('.env')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aedf19e7",
      "metadata": {
        "id": "aedf19e7"
      },
      "outputs": [],
      "source": [
        "autogen.oai.ChatCompletion.start_logging()\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": {\n",
        "            \"gpt-4\",\n",
        "            \"gpt4\",\n",
        "            \"gpt-4-32k\",\n",
        "            \"gpt-4-32k-0314\",\n",
        "            \"gpt-3.5-turbo\",\n",
        "            \"gpt-3.5-turbo-16k\",\n",
        "            \"gpt-3.5-turbo-0301\",\n",
        "            \"chatgpt-35-turbo-0301\",\n",
        "            \"gpt-35-turbo-v0301\",\n",
        "        }\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e7e728",
      "metadata": {
        "id": "e9e7e728"
      },
      "source": [
        "Now, let's import the source code (loading from URL) and also some training examples (defined as string blow)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca962ac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca962ac5",
        "outputId": "4a789991-8ba1-46f3-aad5-7fbaaaff7f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import time\n",
            "from gurobipy import GRB, Model\n",
            "\n",
            "# Example data\n",
            "\n",
            "capacity_in_supplier = {'supplier1': 150, 'supplier2': 50, 'supplier3': 100}\n",
            "\n",
            "shipping_cost_from_supplier_to_roastery = {\n",
            "    ('supplier1', 'roastery1'): 5,\n",
            "    ('supplier1', 'roastery2'): 4,\n",
            ".\n",
            ".\n",
            ".\n",
            "\n",
            "# Solve\n",
            "m.update()\n",
            "model.optimize()\n",
            "\n",
            "print(time.ctime())\n",
            "if m.status == GRB.OPTIMAL:\n",
            "    print(f'Optimal cost: {m.objVal}')\n",
            "else:\n",
            "    print(\"Not solved to optimality. Optimization status:\", m.status)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the source code of our coffee example\n",
        "# code_url = \"https://raw.githubusercontent.com/microsoft/OptiGuide/main/benchmark/application/coffee.py\"\n",
        "code_url = \"https://raw.githubusercontent.com/vineesh12344/Codesprint/main/OptiGuide/benchmark/application/port.py\"\n",
        "response  = requests.get(code_url)\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Get the text content from the response\n",
        "    code = response.text\n",
        "else:\n",
        "    raise RuntimeError(\"Failed to retrieve the file.\")\n",
        "# code = open(code_url, \"r\").read() # for local files\n",
        "\n",
        "\n",
        "# show the first head and tail of the source code\n",
        "print(\"\\n\".join(code.split(\"\\n\")[:10]))\n",
        "print(\".\\n\" * 3)\n",
        "print(\"\\n\".join(code.split(\"\\n\")[-10:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0570952",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # In-context learning examples.\n",
        "# example_qa = \"\"\"\n",
        "# ----------\n",
        "# Question: Why is it not recommended to use just one supplier for roastery 2?\n",
        "# Answer Code:\n",
        "# ```python\n",
        "# z = m.addVars(suppliers, vtype=GRB.BINARY, name=\"z\")\n",
        "# m.addConstr(sum(z[s] for s in suppliers) <= 1, \"_\")\n",
        "# for s in suppliers:\n",
        "#     m.addConstr(x[s,'roastery2'] <= capacity_in_supplier[s] * z[s], \"_\")\n",
        "# ```\n",
        "\n",
        "# ----------\n",
        "# Question: What if there's a 13% jump in the demand for light coffee at cafe1?\n",
        "# Answer Code:\n",
        "# ```python\n",
        "# light_coffee_needed_for_cafe[\"cafe1\"] = light_coffee_needed_for_cafe[\"cafe1\"] * (1 + 13/100)\n",
        "# ```\n",
        "\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31c4b36",
      "metadata": {
        "code_folding": [],
        "id": "e31c4b36"
      },
      "outputs": [],
      "source": [
        "# In-context learning examples.\n",
        "example_qa = \"\"\"\n",
        "----------\n",
        "Question: Why is it not recommended to only use one warehouse for berth 1?\n",
        "Answer Code:\n",
        "```python\n",
        "z = m.addVars(warehouses, vtype=GRB.BINARY, name=\"z\")\n",
        "m.addConstr(sum(z[w] for w in warehouses) <= 1, \"_\")\n",
        "for w in warehouses:\n",
        "    m.addConstr(shipping_costs['Berth1', w] <= warehouseCapacities[w] * z[w], \"_\")\n",
        "```\n",
        "\n",
        "----------\n",
        "Question: What if there's a 50% jump in the capacity of warehouse 1?\n",
        "Answer Code:\n",
        "```python\n",
        "warehouses[\"Warehouse1\"] = warehouses[\"Warehouse1\"] * (1 + 50/100)\n",
        "```\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a5a7d7e",
      "metadata": {
        "id": "5a5a7d7e"
      },
      "source": [
        "Now, let's create an OptiGuide agent and also a user.\n",
        "\n",
        "For the OptiGuide agent, we only allow \"debug_times\" to be 1, which means it can debug its answer once if it encountered errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af53727c",
      "metadata": {
        "id": "af53727c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "agent = OptiGuideAgent(name=\"OptiGuide Coffee Example\",\n",
        "                  source_code=code,\n",
        "                   debug_times=1,\n",
        "                  example_qa=\"\",\n",
        "                llm_config={\n",
        "        \"request_timeout\": 600,\n",
        "        \"seed\": 42,\n",
        "        \"config_list\": config_list,\n",
        "    })\n",
        "\n",
        "user = UserProxyAgent(\"user\", max_consecutive_auto_reply=0,\n",
        "                         human_input_mode=\"NEVER\", code_execution_config=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd615e87",
      "metadata": {
        "id": "bd615e87"
      },
      "source": [
        "Now, let's create a user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a76f67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24a76f67",
        "outputId": "2399e32e-fe3f-429f-9c40-fcc75605741d",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser\u001b[0m (to OptiGuide Coffee Example):\n",
            "\n",
            "What if we prohibit shipping from supplier 1 to roastery 2?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33mOptiGuide Coffee Example\u001b[0m (to writer):\n",
            "\n",
            "\n",
            "Answer Code:\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\OptiGuide\\notebook\\optiguide_example.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/Codesprint/OptiGuide/notebook/optiguide_example.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m message \u001b[39m=\u001b[39m user\u001b[39m.\u001b[39;49minitiate_chat(agent, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat if we prohibit shipping from supplier 1 to roastery 2?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:521\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:324\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[0;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[1;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[0;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:452\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\optiguide\\optiguide.py:136\u001b[0m, in \u001b[0;36mOptiGuideAgent.generate_reply\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Step 2-6: code, safeguard, and interpret\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitiate_chat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writer, message\u001b[39m=\u001b[39;49mCODE_PROMPT)\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success:\n\u001b[0;32m    138\u001b[0m     \u001b[39m# step 7: receive interpret result\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer)[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:521\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:324\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[0;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[1;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[0;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:452\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:764\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[1;32m--> 764\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[0;32m    766\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\agentchat\\conversable_agent.py:596\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    593\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m    595\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    597\u001b[0m     context\u001b[39m=\u001b[39mmessages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_system_message \u001b[39m+\u001b[39m messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mllm_config\n\u001b[0;32m    598\u001b[0m )\n\u001b[0;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\oai\\completion.py:801\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m    800\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[1;32m--> 801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\flaml\\autogen\\oai\\completion.py:206\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m--> 206\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m    207\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:151\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[0;32m    143\u001b[0m         timeout,\n\u001b[0;32m    144\u001b[0m         stream,\n\u001b[0;32m    145\u001b[0m         headers,\n\u001b[0;32m    146\u001b[0m         request_timeout,\n\u001b[0;32m    147\u001b[0m         typed_api_type,\n\u001b[0;32m    148\u001b[0m         requestor,\n\u001b[0;32m    149\u001b[0m         url,\n\u001b[0;32m    150\u001b[0m         params,\n\u001b[1;32m--> 151\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__prepare_create_request(\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:108\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    106\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 108\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[0;32m    109\u001b[0m     api_key,\n\u001b[0;32m    110\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[0;32m    111\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[0;32m    112\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[0;32m    113\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    117\u001b[0m     deployment_id,\n\u001b[0;32m    118\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     params,\n\u001b[0;32m    127\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\openai\\api_requestor.py:139\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    132\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[0;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
            "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\Codesprint\\env\\lib\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
            "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
          ]
        }
      ],
      "source": [
        "message = user.initiate_chat(agent, message=\"What if we prohibit shipping from supplier 1 to roastery 2?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdd1f28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbdd1f28",
        "outputId": "c5e943cf-fce7-4484-8cd8-433d4fede8e5",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user (to OptiGuide Coffee Example):\n",
            "\n",
            "What is the impact of supplier1 being able to supply only half the quantity at present?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "OptiGuide Coffee Example (to writer):\n",
            "\n",
            "\n",
            "Answer Code:\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "writer (to OptiGuide Coffee Example):\n",
            "\n",
            "```python\n",
            "# Update the capacity of supplier1 to half the original value\n",
            "new_capacity = capacity_in_supplier['supplier1'] / 2\n",
            "capacity_in_supplier.update({'supplier1': new_capacity})\n",
            "\n",
            "# Update the supply constraint for supplier1\n",
            "model.remove(model.getConstrByName('supply_supplier1'))\n",
            "model.addConstr(\n",
            "    sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
            "        if i[0] == 'supplier1') <= capacity_in_supplier['supplier1'], \"supply_supplier1\")\n",
            "\n",
            "# Solve the updated model\n",
            "model.optimize()\n",
            "\n",
            "# Print the new optimal cost\n",
            "if model.status == GRB.OPTIMAL:\n",
            "    updated_cost = model.objVal\n",
            "else:\n",
            "    updated_cost = None\n",
            "\n",
            "updated_cost\n",
            "```\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "OptiGuide Coffee Example (to safeguard):\n",
            "\n",
            "\n",
            "--- Code ---\n",
            "# Update the capacity of supplier1 to half the original value\n",
            "new_capacity = capacity_in_supplier['supplier1'] / 2\n",
            "capacity_in_supplier.update({'supplier1': new_capacity})\n",
            "\n",
            "# Update the supply constraint for supplier1\n",
            "model.remove(model.getConstrByName('supply_supplier1'))\n",
            "model.addConstr(\n",
            "    sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
            "        if i[0] == 'supplier1') <= capacity_in_supplier['supplier1'], \"supply_supplier1\")\n",
            "\n",
            "# Solve the updated model\n",
            "model.optimize()\n",
            "\n",
            "# Print the new optimal cost\n",
            "if model.status == GRB.OPTIMAL:\n",
            "    updated_cost = model.objVal\n",
            "else:\n",
            "    updated_cost = None\n",
            "\n",
            "updated_cost\n",
            "\n",
            "--- One-Word Answer: SAFE or DANGER ---\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "safeguard (to OptiGuide Coffee Example):\n",
            "\n",
            "SAFE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n",
            "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
            "Model fingerprint: 0xc28a6742\n",
            "Variable types: 0 continuous, 18 integer (0 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+00]\n",
            "  Objective range  [2e+00, 1e+01]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [2e+01, 2e+02]\n",
            "Found heuristic solution: objective 2900.0000000\n",
            "Presolve time: 0.00s\n",
            "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
            "Variable types: 0 continuous, 18 integer (0 binary)\n",
            "Found heuristic solution: objective 2896.0000000\n",
            "\n",
            "Root relaxation: objective 2.470000e+03, 11 iterations, 0.00 seconds (0.00 work units)\n",
            "\n",
            "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
            " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
            "\n",
            "*    0     0               0    2470.0000000 2470.00000  0.00%     -    0s\n",
            "\n",
            "Explored 1 nodes (11 simplex iterations) in 0.03 seconds (0.00 work units)\n",
            "Thread count was 2 (of 2 available processors)\n",
            "\n",
            "Solution count 3: 2470 2896 2900 \n",
            "\n",
            "Optimal solution found (tolerance 1.00e-04)\n",
            "Best objective 2.470000000000e+03, best bound 2.470000000000e+03, gap 0.0000%\n",
            "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n",
            "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
            "Model fingerprint: 0x5d165816\n",
            "Variable types: 0 continuous, 18 integer (0 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+00]\n",
            "  Objective range  [2e+00, 1e+01]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [2e+01, 1e+02]\n",
            "\n",
            "MIP start from previous solve did not produce a new incumbent solution\n",
            "MIP start from previous solve violates constraint supply_supplier1 by 5.000000000\n",
            "\n",
            "Presolve time: 0.00s\n",
            "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
            "Variable types: 0 continuous, 18 integer (0 binary)\n",
            "\n",
            "Root relaxation: infeasible, 14 iterations, 0.00 seconds (0.00 work units)\n",
            "\n",
            "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
            " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
            "\n",
            "     0     0 infeasible    0               - infeasible      -     -    0s\n",
            "\n",
            "Explored 1 nodes (14 simplex iterations) in 0.14 seconds (0.00 work units)\n",
            "Thread count was 2 (of 2 available processors)\n",
            "\n",
            "Solution count 0\n",
            "\n",
            "Model is infeasible\n",
            "Best objective -, best bound -, gap -\n",
            "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n",
            "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
            "Model fingerprint: 0x5d165816\n",
            "Variable types: 0 continuous, 18 integer (0 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+00]\n",
            "  Objective range  [2e+00, 1e+01]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [2e+01, 1e+02]\n",
            "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
            "\n",
            "Continuing optimization...\n",
            "\n",
            "\n",
            "Explored 1 nodes (14 simplex iterations) in 0.01 seconds (0.00 work units)\n",
            "Thread count was 2 (of 2 available processors)\n",
            "\n",
            "Solution count 0\n",
            "\n",
            "Model is infeasible\n",
            "Best objective -, best bound -, gap -\n",
            "Wed Sep 20 21:43:34 2023\n",
            "Not solved to optimality. Optimization status: 3\n",
            "\n",
            "Computing Irreducible Inconsistent Subsystem (IIS)...\n",
            "\n",
            "           Constraints          |            Bounds           |  Runtime\n",
            "      Min       Max     Guess   |   Min       Max     Guess   |\n",
            "--------------------------------------------------------------------------\n",
            "        0        11         -         0        18         -           0s\n",
            "       11        11        11         0         0         0           0s\n",
            "\n",
            "IIS computed: 11 constraints, 0 bounds\n",
            "IIS runtime: 0.01 seconds (0.00 work units)\n",
            "infeasible\n",
            "Conflicting Constraints:\n",
            "['flow_roastery1', 'flow_roastery2', 'supply_supplier3', 'supply_supplier2', 'light_demand_cafe2', 'dark_demand_cafe2', 'light_demand_cafe1', 'dark_demand_cafe1', 'light_demand_cafe3', 'dark_demand_cafe3', 'supply_supplier1']\n",
            "OptiGuide Coffee Example (to writer):\n",
            "\n",
            "Here are the execution results: infeasible\n",
            "Conflicting Constraints:\n",
            "['flow_roastery1', 'flow_roastery2', 'supply_supplier3', 'supply_supplier2', 'light_demand_cafe2', 'dark_demand_cafe2', 'light_demand_cafe1', 'dark_demand_cafe1', 'light_demand_cafe3', 'dark_demand_cafe3', 'supply_supplier1']\n",
            "\n",
            "Can you organize these information to a human readable answer?\n",
            "Remember to compare the new results to the original results you obtained in the\n",
            "beginning.\n",
            "\n",
            "--- HUMAN READABLE ANSWER ---\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "writer (to OptiGuide Coffee Example):\n",
            "\n",
            "The original optimal cost of the coffee distribution problem was 2470.0. However, when we reduced the capacity of supplier1 to half of its initial value, the problem became infeasible. This means there is no solution that can fulfill all the constraints given the new capacity of supplier1.\n",
            "\n",
            "Here's the list of constraints that are conflicting in the infeasible case:\n",
            "1. flow_roastery1\n",
            "2. flow_roastery2\n",
            "3. supply_supplier1\n",
            "4. supply_supplier2\n",
            "5. supply_supplier3\n",
            "6. light_demand_cafe1\n",
            "7. dark_demand_cafe1\n",
            "8. light_demand_cafe2\n",
            "9. dark_demand_cafe2\n",
            "10. light_demand_cafe3\n",
            "11. dark_demand_cafe3\n",
            "\n",
            "Reducing the capacity of supplier1 has a negative impact on the supply chain such that it's not possible to satisfy all the demand of light and dark coffee at each cafe under the given constraints.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "OptiGuide Coffee Example (to user):\n",
            "\n",
            "The original optimal cost of the coffee distribution problem was 2470.0. However, when we reduced the capacity of supplier1 to half of its initial value, the problem became infeasible. This means there is no solution that can fulfill all the constraints given the new capacity of supplier1.\n",
            "\n",
            "Here's the list of constraints that are conflicting in the infeasible case:\n",
            "1. flow_roastery1\n",
            "2. flow_roastery2\n",
            "3. supply_supplier1\n",
            "4. supply_supplier2\n",
            "5. supply_supplier3\n",
            "6. light_demand_cafe1\n",
            "7. dark_demand_cafe1\n",
            "8. light_demand_cafe2\n",
            "9. dark_demand_cafe2\n",
            "10. light_demand_cafe3\n",
            "11. dark_demand_cafe3\n",
            "\n",
            "Reducing the capacity of supplier1 has a negative impact on the supply chain such that it's not possible to satisfy all the demand of light and dark coffee at each cafe under the given constraints.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "user.initiate_chat(agent, message=\"What is the impact of supplier1 being able to supply only half the quantity at present?\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
